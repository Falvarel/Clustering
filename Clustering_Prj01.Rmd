---
title: "PROYECTO CLUSTERING"
author: "Federico Álvarez-Labrador"
date:
output:
  html_document: default
  word_document: default
  pdf_document: default
---

### 1. INTRODUCCIÓN Y DESGLOSE DE LIBRERÍAS

El dataset analizado en la presente práctica recoge los datos de 7 variables que corresponden a indicadores de criminalidad por 10.000 habitantes para cada uno de los 50 estados de Estados Unidos.

Los indicadores de criminalidad son los siguientes:

  1. Asesinatos (variable MURDER)
  
  2. Violaciones (RAPE)
  
  3. Robos (ROBBERY)
  
  4. Asaltos (ASSAULT)
  
  5. Agresiones (BURGLARY)
  
  6. Hurtos (LARCENY)
  
  7. Robos de coches (AUTO_THEFT)
  

El objetivo de la presente práctica es realizar un análisis cluster agrupando estados que, respecto de dichas variables, tengan un comportamiento parecido.

Para la realización de esta práctica se han empleado diez paquetes de R que se enumeran a continuación: 

- DPLYR - *https://cran.r-project.org/web/packages/dplyr/index.html*
- GGPLOT2 - *https://cran.r-project.org/web/packages/ggplot2/index.html*
- VIM - *https://cran.r-project.org/web/packages/VIM/index.html*
- GRIDEXTRA - *https://cran.r-project.org/web/packages/gridExtra/index.html*
- CORRPLOT - *https://cran.r-project.org/web/packages/corrplot/index.html*
- VEGAN - *https://cran.r-project.org/web/packages/vegan/index.html*
- SQLDF - *https://cran.r-project.org/web/packages/sqldf/index.html*
- FMSB - *https://cran.r-project.org/web/packages/fmsb/index.html*
- NBCLUST - *https://cran.r-project.org/web/packages/NbClust/index.html*
- FACTOEXTRA - *https://cran.r-project.org/web/packages/factoextra/index.html*

```{r, include=FALSE, result="hide", warning=TRUE}

library(dplyr)
library(ggplot2)
library(VIM)
library(gridExtra)
library(corrplot)
library(vegan)
library(sqldf)
library(fmsb)
library(NbClust)
library(factoextra)

```

***

### 2. TRATAMIENTO DE VARIABLES  

Empezamos cargando el dataset base (archivo "crime.csv") incluido en la documentación  correspondiente a esta práctica. Una vez cargado el dataset, lo asignamos a una variable creando un dataframe para utilizarlo según sea más conveniente para el análisis posterior:

```{r, include=TRUE, result="hold", warning=TRUE}

crime_df <- read.csv("crime.csv", header=T, sep=" ", dec=".", stringsAsFactors=F)

```

Revisamos los datos cargados:
```{r, include=TRUE, result="hold", warning=TRUE, out.width=1}

# Dimensiones del dataset
dim(crime_df)
# Clase y muestra de cada variable
str(crime_df)

```

Revisados los datos y analizando el dataframe resultante parece que los mismos están organizados y se han cargado correctamente (cabecera de la database, separador de los datos, nombres y formatos de cada variable, decimales...). 

Dado que las todas las variables son numéricas continuas salvo la primera que es categórica, vamos a utilizar dicha variable para nombrar las observaciones y dejar el resto de variables con los valores numéricos. 

```{r, include=TRUE, result="hold", warning=TRUE, out.width=1}

crime_df1 <- crime_df %>%
  select(-c("State"))

rownames(crime_df1) <- crime_df$State

# Clase y muestra de cada variable
str(crime_df1)
# Nombres de las observaciones
rownames(crime_df1)[1:10]

```

Una vez cargados y revisados los datos, el siguiente paso será analizar a fondo los mismos para detectar la presencia y hacer un tratamiento de los valores "missing" y "outliers".


#### 2.1. ANÁLISIS Y TRATAMIENTO DE MISSING VALUES

&nbsp;

El dataset contiene 50 observaciones (los 50 estados de Estados Unidos) y 8 variables (la variable "State" que identifica el estado y las otras 7 que corresponden a los indicadores de criminalidad). 

```{r, include=TRUE, result="hold", warning=TRUE}

dim(crime_df1)

```

Revisamos los "missing values" restantes. Analizamos el dataset para determinar su distribución (a qué variables afecta y cuánto) y tomar decisiones sobre qué hacer con ellos en cada caso.

```{r, include=FALSE, result="hold", warning=TRUE}

crime_nas <- colSums(is.na(crime_df1))
sum(crime_nas)

```

No tenemos missing values en ninguna de las variables, como podemos observar también en el siguiente gráfico:

```{r, include=TRUE, result="hold", warning=TRUE, fig.height=3.9}

crime_nas2 <- crime_df1 %>% 
  select(names(crime_df[crime_df!=0]))
aggr(crime_df1, prop=c(TRUE,TRUE), col=c("SteelBlue", "firebrick1", "LightGrey"), numbers=F,
     sortVar=TRUE, cex.axis=0.3)

```

En caso de tener missing values habría que analizar su tratamiento (si es más idóneo descartar las observaciones o imputar los valores), pero al no tener ninguno no es necesario.


#### 2.2. ANÁLISIS Y TRATAMIENTO DE OUTLIERS  

&nbsp;

Una vez hemos organizado y preparado el dataset, pasamos a analizar los estadísticos básicos de cada variable para tratar de encontrar dispersiones altas o outliers entre las variables del mismo. Preparo el dataset y separo aquellas variables que no son relevantes (en este caso la primera variable "State" que es categórica).

```{r, include=TRUE, result="hold", warning=TRUE}

str(crime_df1)

```

&nbsp;

Obtenemos los estadísticos básicos de todas las variables (nos fijaremos principalmente en: mínimo, máximo, media y mediana) y revisamos los posibles outliers o valores erróneos del dataset.

```{r, include=TRUE, result="hold", warning=TRUE}

summary(crime_df1)

```

En el primer análisis numérico de los datos se observan algunos máximos altos dentro de las distribuciones de sus variables, pero no se observan variables con dispersiones de valores especialmente grandes o valores mínimos/máximos muy separados de las medias y medianas de dichas variables. Éstos valores pueden constituir los posibles outliers o valores erróneos que estamos buscando, para evitar fallos en el análisis posterior de los datos.


&nbsp;


Para seguir analizando esos valores vamos a visualizar las distribuciones de valores de todas las variables de nuestro dataset, resaltando los posibles outliers.

```{r, include=FALSE, result="hide", warning=TRUE}

df_varsboxplots <- function(df, x_vars, y_var=NULL, sep=F, nrows, ncols) {
  plot_list=list()
  nm <- x_vars
  if (sep==F){
    for (i in seq_along(nm)) {
      ggp <- ggplot(df,aes_string(x=nm[i], y=y_var), cex.axis=0.01) + 
        geom_boxplot(fill = "SteelBlue", outlier.colour="firebrick1", outlier.alpha = 0.5) +
        theme(axis.title.x = element_text(color = "black", size = 10, face = "bold"),
              axis.title.y = element_text(color = "black", size = 10, face = "bold"))
      plot_list[[i]] <- ggp
    }
    grid.arrange(grobs=plot_list)
  }
  else{
    for (i in seq_along(nm)) {
      print(ggplot(df,aes_string(x=nm[i], y=y_var), cex.axis=0.01) + 
              geom_boxplot(fill = "SteelBlue", outlier.colour="firebrick1", outlier.alpha = 0.5) + theme(axis.title.x = element_text(color = "black", size = 10, face = "bold"), axis.title.y = element_text(color = "black", size = 10, face = "bold")))
    }
  }
}

```



```{r, include=TRUE, result="hold", warning=TRUE, fig.height=4}

df_varsboxplots(crime_df1, c("Murder", "Rape"), nrows=2, ncols=1)

df_varsboxplots(crime_df1, c("Robbery", "Assault"), nrows=2, ncols=1)

df_varsboxplots(crime_df1, c("Burglary", "Larceny"), nrows=2, ncols=1)


```

```{r, include=TRUE, result="hold", warning=TRUE, fig.height=2.5}

df_varsboxplots(crime_df1, c("Auto_Theft"), nrows=2, ncols=1)


```

&nbsp;

Como podemos observar en los gráficos, salvo en las 2 primeras variables ("Murder" y "Rape"), el resto sí que tienen valores que se alejan de la distribución de la mayoría de los valores de dicha variable.

La ausencia de outliers en las 2 primeras variables podría deberse a que los crímenes que representan estas variables son los de mayor gravedad ("Asesinato" y "Violación") y no hay grandes diferencias entre los distintos estados (la distribución de estos crímenes es más homogénea en todo el país).


&nbsp;


Analizamos los gráficos de las variables con presencia de outliers:

- Variable "Robbery" (relativa a robos con violencia)

```{r, include=TRUE, result="hold", warning=TRUE}

crime_out1 <- crime_df1 %>% 
  select(c("Robbery")) %>%
  arrange(rank=rank(Robbery))
head(crime_out1,5)
tail(crime_out1,5)

```


&nbsp;

- Variable "Assault" (relativa a agresiones a personas)

```{r, include=TRUE, result="hold", warning=TRUE}

crime_out2 <- crime_df1 %>% 
  select(c("Assault")) %>%
  arrange(rank=rank(Assault))
head(crime_out2,5)
tail(crime_out2,5)

```

&nbsp;

- Variable "Burglary" (relativa a allanamientos de propiedades privadas con o sin hurto y/o violencia)

```{r, include=TRUE, result="hold", warning=TRUE}

crime_out3 <- crime_df1 %>% 
  select(c("Burglary")) %>%
  arrange(rank=rank(Burglary))
head(crime_out3,5)
tail(crime_out3,5)

```

&nbsp;

- Variable "Larceny" (relativa relativa a robos sin violencia)

```{r, include=TRUE, result="hold", warning=TRUE}

crime_out4 <- crime_df1 %>% 
  select(c("Larceny")) %>%
  arrange(rank=rank(Larceny))
head(crime_out4,5)
tail(crime_out4,5)

```

&nbsp;

- Variable "Auto_Theft" (relativa a robos de vehículos)

```{r, include=TRUE, result="hold", warning=TRUE}

crime_out5 <- crime_df1 %>% 
  select(c("Auto_Theft")) %>%
  arrange(rank=rank(Auto_Theft))
head(crime_out5,5)
tail(crime_out5,5)

```


Del análisis anterior podemos interpretar que los valores extremos que observamos en algunas de las variables no se pueden considerar "outliers" o valores erróneos.

Como se puede ver en los extractos ordenados de las distintas variables, los valores extremos en dichas variables se debe a las diferencias de ciertos estados.


#### 2.3. RELACIONES ENTRE VARIABLES 

##### *Matrices de Covarianzas y de Correlación*

- Matriz de covarianzas (S):

```{r, include=TRUE, result="asis", warning=TRUE}

S <- cov(crime_df1)
paste("Det(S)=",det(S), " / ", "Sum(diag(S))=",sum(diag(S)), sep=" ")

```

- Matriz de correlaciones (R):

```{r, include=TRUE, result="hold", warning=TRUE}

R <- cor(crime_df1)
paste("Det(R)=",det(R), " / ", "Sum(diag(R))=",sum(diag(R)), sep=" ")

```

Las matrices anteriores nos sirven para hacernos una idea de lo homogéneos/heterogéneos que son los datos y de lo correlacionados (relaciones entre variables) o no que están los datos.

En este caso como las variables tienen unidades diferentes, se va a emplear la matriz de correlaciones para el análisis. El determinante de la matriz de correlación es próximo a cero por lo que podemos asumir que existen relaciones entre variables (éstas pueden ser lineales o no).


&nbsp;


Preparamos una visualización de la matriz de correlación para analizar de forma más sencilla las variables relacionadas, cantidad de relaciones y fuerza de las mismas. Existen varios métodos para calcular las correlaciones entre variables, en este caso vamos a utilizar dos:

- Método de "Pearson"_ Analiza correlaciones de tipo lineal, cuando existe relación lineal entre variables.

- Método de "Spearman"_ Analiza correlaciones de tipo rango, cuando existe relación entre el crecimiento de una variable y a la vez el de otra, y viceversa (no la relación lineal solamente).


```{r, include=TRUE, result="hold", warning=TRUE, fig.height=12, fig.width=25}

corr_01 <- cor(crime_df1, method = c("pearson"))
corr_02 <- cor(crime_df1, method = c("spearman"))

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

par(mfrow=c(1,2))
corrplot_01 <- corrplot(corr_01, method="color", type="lower", 
                        tl.col="black",diag=FALSE, col=col(200), 
                        addCoef.col = "black", tl.cex=2, number.cex=2, cl.cex=2)
mtext("\n\n Matriz de correlación \n (método de 'Pearson')", at=3.55, line=-5, cex=2.75)

corrplot_02 <- corrplot(corr_02, method="color", type="lower", 
                        tl.col="black",diag=FALSE, col=col(200), 
                        addCoef.col = "black", tl.cex=2, number.cex=2, cl.cex=2)
mtext("\n\n Matriz de correlación \n (método de 'Spearman')", at=3.55, line=-5, cex=2.75)

```

Este gráfico es más útil que los scatterplots separados. Nos fijamos en los valores azules/rojos oscuros, son aquellos con relaciones fuertes. 


&nbsp;


De todas maneras hay  que tener en cuenta que se están analizando las posibles relaciones lineales, pudiendo haber variables en el gráfico con apenas relación lineal con otras variables, pero pudiendo tener una relación no lineal con éstas.

Al utilizar los métodos de "Pearson" y "Spearman" podemos comprobar las posibles relaciones lineales como no lineales entre variables. Tras revisar los gráficos de correlación observamos que existen relaciones considerables entre algunas variables.

En este caso como criterio se ha decidido reducir la correlación entre variables descartando aquellas más correlacionadas con el resto de variables, tratando de reducir al máximo la correlación en las variables restantes.

Después de revisar las correlaciones (tanto lineales como no lineales), las variables descartadas son:

- Variable "Rape"_ variable muy correlacionada con "Robbery", "Assault" y "Burglary"
- Variable "Burglary"_ variable muy correlacionada con "Rape", "Robbery", "Larcery" y "Auto_Theft".

```{r, include=TRUE, result="hold", warning=TRUE, fig.height=25, fig.width=25}

crime_df2 <- crime_df1 %>% 
  select(-c("Rape","Burglary"))

```



#### 2.4. TRANSFORMACIÓN DE LOS DATOS  

&nbsp;

Reviso las variables restantes y analizo si es necesario realizar alguna transformación.

```{r, include=TRUE, result="hold", warning=TRUE}

str(crime_df2)
summary(crime_df2)

```


En el presente caso, todas las variables a analizar son variables numéricas continuas. Las variables continuas pueden requerir en algunos casos, una transformación previa al proceso de clustering. La transformación más popular es la estandarización ya que con ello se evita la influencia de
la unidad de medida. 


<!-- Es un hecho que variables con alto poder discriminante pueden ver mermada su capacidad diferenciadora tras una estandarización. No obstante, cuando las variables usadas vengan en la misma escala, tales como por ejemplo: si todas las variables se refieren a porcentajes, etc, no es aconsejable la estandarización.  -->

<!-- Ésta se reserva, fundamentalmente, cuando se observa que, por ejemplo, unas determinadas variables pueden tener un peso mayor que otras, simplemente porque la unidad de medida en que aparecen dan lugar a puntuaciones con valores relativamente altos en comparación con los de las otras, de tal modo, que pueden, incluso, llegar a anular la influencia de otras hasta el extremo de que dé igual incluirlas o no.  -->


Las variables de los datos, pese a representar sucesos diferentes utilizan unidades similares, midiendo el número de dichos sucesos por cada 10.000 habitantes por lo que podemos considerar dichas unidades como equivalentes. Aún teniendo en cuenta este hecho, las variables que tienen un mayor rango de variación tienden a tener más importancia a la hora de conformar los clusters por ello se van estandarizar todas las variables.


```{r, include=TRUE, result="hold", warning=TRUE}

crime_df3 <- as.data.frame(scale(crime_df2, center = FALSE,
                                 scale = apply(crime_df2, 2, sd, na.rm = TRUE)))
str(crime_df3)

```


#### 2.5. DATOS PREPARADOS 

&nbsp;

Teniendo en cuenta todo el análisis anterior, definimos los datos finales a utilizar en el clustering.

```{r, include=TRUE, result="hold", warning=TRUE, fig.height=25, fig.width=25}

str(crime_df3)
summary(crime_df3)

```


&nbsp;


*** 


### 3. ANÁLISIS CLUSTER

#### 3.1. SELECCIÓN DE LA MÉTRICA

&nbsp;

Para realizar el clustering de los datos es preciso determinar cómo se medirá la distancia entre los elementos. En este caso vamos a utilizar la distancia "Euclídea".
$d(u,v)=\sqrt{(u1-u2)^2+(v1-v2)^2}$

```{r, include=TRUE, result="hold", warning=TRUE, fig.height=25, fig.width=25}

matrizDistancias <- vegdist(crime_df3, method = "euclidean")

```


#### 3.2. SELECCIÓN DEL ALGORITMO

&nbsp;

Dado el tamaño pequeño de los datos, se podría emplear directamente un método jerárquico que, aunque más costoso computacionalmente (se suele aplicar a una muestra de los datos), permite obtener clasificaciones más cercanas a la óptima.

De todas maneras, con fines prácticos, para el presente análisis es preciso aclarar que se va a seguir el método "bietápico". En este método el preocedimiento a seguir es el siguiente:

  - **i.** Aplicar un algoritmo "jerárquico" (obteniendo el número "K" de clusters y los centroides de partida)

  - **ii.** Aplicar un algoritmo de "optimización" (utilizando los resultados anteriores)

Para el método de "optimización" necesitamos definir el número de clusters en el que vamos a dividir los elementos y los centroides de dichos clusters (ya que son métodos muy sensibles a los centroides de partida). Para ello utilizaremos los datos obtenidos de la aplicación del método "jerárquico".

Vamos a realizar una comprobación adicional calculando la distribución porcentual de elementos entre los clusters obtenidos del método "jerárquico" y comparándola con la resultante al aplicar el método de "optimización" (si el procedimiento se ha realizado correctamente, deberían ser muy similares). 



#### 3.3. MÉTODO JERÁRQUICO

&nbsp;

Los métodos jerárquicos suelen tener un coste computacional alto por lo que es recomendable no aplicarlos a bases de datos muy grandes (por necesidad de capacidad y tiempo de procesamiento). Dado que los datos con los que estamos trabajando tienen tan sólo 50 observaciones, no se considera necesario extraer una muestra para aplicar el método jerárquico.


##### ***Método jerárquico (distancias entre grupos mediante el método "WARD")***

&nbsp;

El método aglomerativo a utilizar va a ser uno de tipo disgregativo o de división. Éstos métodos parten del conjunto formado por todos los elementos y, en cada iteración, realizan una separación dando lugar a clusters más pequeños). 

En los métodos jerárquicos se debe definir también la distancia entre grupos, para el presente análisis se va a utilizar a su vez el Método "WARD". Éste método busca agrupar los elementos buscando el mínimo incremento de "varianza intracluster" (desventajas: sensible a "outliers" y poco eficiente computacionalmente; ventaja: capaz de acercarse más a la clasificación óptima).


```{r, include=TRUE, result="hold", warning=TRUE, fig.height=25, fig.width=25}

clusterJer <- hclust(matrizDistancias, method="ward.D2",)
clusterJer_labs <- rownames(crime_df3)

```


Para visualizar mejor los clusters generados en cada iteración y su influencia en la "varianza intracluster", representamos el dendrograma asociado al método aplicado:

```{r, include=TRUE, result="hold", warning=TRUE, fig.height=23, fig.width=25}

plot(x=clusterJer, labels=clusterJer_labs, main="Dendrograma \n 'Crime Dataset Clustering'",
     xlab="States (United States of America)" , ylab= "Varianza Intracluster")
rect.hclust(clusterJer, k=2, border="azure3")
rect.hclust(clusterJer, k=3, border="lightblue3")
rect.hclust(clusterJer, k=4, border="darkred") 
rect.hclust(clusterJer, k=5, border="red") 
rect.hclust(clusterJer, k=6, border="orange") 
rect.hclust(clusterJer, k=7, border="pink")
rect.hclust(clusterJer, k=8, border="yellow2") 
rect.hclust(clusterJer, k=9, border="green") 
rect.hclust(clusterJer, k=10, border="springgreen3")
rect.hclust(clusterJer, k=11, border="dodgerblue")
rect.hclust(clusterJer, k=12, border="blue") 
rect.hclust(clusterJer, k=13, border="darkmagenta")
rect.hclust(clusterJer, k=14, border="darkorchid1")
rect.hclust(clusterJer, k=15, border="magenta1")

```


Después de revisar el dendrograma obtenido y de acuerdo a los valores de varianza intracluster, se puede observar que la disminución de la misma se reduce en gran medida a partir de K=4 (4 clusters). Además de este dendrograma, vamos a aplicar otros dos métodos alternativos para definir el número más óptimo de clusters para la segregación del dataset.


##### ***Método de Elbow (comprobación adicional)***

&nbsp;

No forma parte del método bietápico pero también podemos emplear el método de Elbow para definir el número de clusters, que evalúa la varianza intracluster en función del número de clusters.


```{r, include=TRUE, result="hold", warning=TRUE, fig.height=5, fig.width=8}

# Calcular SSW (Sum of Squares Wihin) para distintos n?mero de grupos
set.seed(12345)
n <- dim(crime_df3)[1] # Numero de registros
p <- dim(crime_df3)[2] # Numero de variables

SSW <- (n - 1) * sum(apply(crime_df3,2,var)) 

# Se aplica el metodo k-means con 5 inicializaciones distintas de centroides
# para que no sea tan sensible a los centroides de partida
for (i in 2:30) SSW[i] <- 
  sum(kmeans(crime_df3,centers=i,nstart=3,iter.max=20)$withinss)

# Metodo de Elbow
plot(1:30, SSW, type="b", xlab="Number of Clusters", 
     ylab="Sum of squares within groups",pch=19, col="steelblue4")

```

Para determinar el número óptimo de clusters se debe buscar el "codo" en el gráfico, que da a entender que la consideración de un cluster adicional no mejora mucho la segmentación (en ocasiones se trata de un criterio subjetivo). En este caso, el "codo" podría considerarse en torno a 6 clusters pero no queda muy claro. 


##### ***Función "NbClust" (comprobación adicional)***

&nbsp;

La segunda alternativa que vamos a emplear es aplicar la función "NbClust". La función "NbClust" toma los datos y tras definir una serie de parámetros, como la distancia entre elementos a utilizar y un rango de valores para los clusters buscados, aplica 30 índices para determinar los nº de clusters más óptimos. Como resultado, proporciona para los nº de cluster en el rango definido en la función el número de índices que lo consideran como el nº óptimo de clusters.

Esta función no debe considerarse como un resultado absoluto o definitivo, si no como una ayuda para visualizar los posibles nº óptimos de clusters que se deberían considerar. Tampoco se deben olvidar nunca los posibles criterios solicitados por un cliente o las condiciones que se consideren oportunas en el análisis y que pueden llegar a condicionar dicho análisis (también determinar el orden del nº de clusters).

&nbsp;

Por tanto, aplicamos la función "NbClust" con un primer rango de nº de clusters amplio (n=[2;20]) para determinar cuáles pueden ser los números óptimos de los mismos.

```{r, include=TRUE, result="hold", warning=FALSE, fig.width=8, fig.height=5.5}

nbclust_01 <- NbClust(crime_df3, distance = "euclidean", min.nc = 2,
                       max.nc = 20, method = "complete", index ="all")

```

&nbsp;

```{r, include=TRUE, result="hold", warning=FALSE, fig.width=8, fig.height=5.5}

factoextra::fviz_nbclust(nbclust_01) + theme_minimal() + 
  ggtitle("NbClust's optimal number of clusters")

```

Los nº de clusters que obtienen los mayores números de indicadores son: 2, 6 y 20. Teniendo en cuenta el orden de magnitud del dataset (número de observaciones) se considera que no tendría mucho sentido hacer una división del mismo en 20 clusters, muchos de los cuales tendrían 1 ó 2 elementos tan sólo.

&nbsp;

En base a los resultados anteriores y a la consideración tomada, se vuelve a aplicar la función "NbClust" pero sobre un rango más acotado del nº de clusters a obtener (n=[2;8]) y se analizan los resultados.

```{r, include=TRUE, result="hold", warning=FALSE, fig.width=8, fig.height=5.5}

nbclust_02 <- NbClust(crime_df3, distance = "euclidean", min.nc = 2,
                       max.nc = 8, method = "complete", index ="all")

```

&nbsp;

```{r, include=TRUE, result="hold", warning=FALSE, fig.width=8, fig.height=5.5}

factoextra::fviz_nbclust(nbclust_02) + theme_minimal() + 
  ggtitle("NbClust's optimal number of clusters")

```

Los nº de clusters que obtienen los mayores números de indicadores son: 2 y 6. Estos números se repiten de nuevo respecto a la primera iteración y destacan considerablemente respecto al resto de opciones, por ello los tomaremos para hacer 2 segregaciones alternativas del dataset y comparar sus resultados.


&nbsp;


##### ***Caso 1: 2 clusters (k=2)***

&nbsp;

Supongamos que se decide dividir todos los elementos en 2 clusters. Definimos el dataframe con una nueva columna que recoge el cluster al que pertenece cada observación.

```{r, include=TRUE, result="hold", warning=TRUE}

asignJer_k2 <- cbind(crime_df3, cutree(clusterJer, k=2))

colnames(asignJer_k2)[6] <- "Cluster_k2"

str(asignJer_k2)

```
&nbsp;

Calculamos los centroides asociados a cada uno de los grupos jerárquicos (clusters):

```{r, include=TRUE, result="hold", warning=TRUE}

centroJer_k2 <- sqldf("SELECT Cluster_k2,
                count(*) AS tamano_Cluster,
                avg(Murder) AS Murder,
                avg(Robbery) AS Robbery,
                avg(Assault) AS Assault,
                avg(Larceny) AS Larceny,
                avg(Auto_Theft) AS Auto_Theft
                FROM asignJer_k2
                GROUP BY Cluster_k2")

print.data.frame(centroJer_k2)

```

&nbsp;

Calculamos la distribución porcentual de los elementos en cada grupo jerárquico (cluster):

```{r, include=TRUE, result="hold", warning=TRUE}

tam_Cluster_k2 <- centroJer_k2[2]
tam_Cluster_k2

perc_tamJer_Cluster_k2 <- centroJer_k2[2]/sum(centroJer_k2[2])
perc_tamJer_Cluster_k2

```


&nbsp;


##### ***Caso 2: 6 clusters (k=6)***

&nbsp;

Supongamos que se decide dividir todos los elementos en 6 clusters. Definimos el dataframe con una nueva columna que recoge el cluster al que pertenece cada observación.

```{r, include=TRUE, result="hold", warning=TRUE}

asignJer_k6 <- cbind(crime_df3, cutree(clusterJer, k=6))

colnames(asignJer_k6)[6] <- "Cluster_k6"

str(asignJer_k6)

```
&nbsp;

Calculamos los centroides asociados a cada uno de los grupos jerárquicos (clusters):

```{r, include=TRUE, result="hold", warning=TRUE}

centroJer_k6 <- sqldf("SELECT Cluster_k6,
                count(*) AS tamano_Cluster,
                avg(Murder) AS Murder,
                avg(Robbery) AS Robbery,
                avg(Assault) AS Assault,
                avg(Larceny) AS Larceny,
                avg(Auto_Theft) AS Auto_Theft
                FROM asignJer_k6
                GROUP BY Cluster_k6")

print.data.frame(centroJer_k6)

```

&nbsp;

Calculamos la distribución porcentual de los elementos en cada grupo jerárquico (cluster):

```{r, include=TRUE, result="hold", warning=TRUE}

tam_Cluster_k6 <- centroJer_k6[2]
tam_Cluster_k6

```

&nbsp;

```{r, include=TRUE, result="hold", warning=TRUE}

perc_tamJer_Cluster_k6 <- centroJer_k6[2]/sum(centroJer_k6[2])
perc_tamJer_Cluster_k6

```

&nbsp;

#### 3.4. MÉTODO DE OPTIMIZACIÓN

&nbsp;

El método de optimización a utilizar va a ser el método "Kmeans". Los métodos de optimización tienen menor coste computacional y son más rápidos pero presuponen el número de clusters a generar. Además son métodos muy sensibles a los centroides de partida.

Para evitar estos inconvenientes el método bietápico utiliza el número de clusters y los centroides de los mismos, calculados en el método jerárquico anterior, como información de partida para el método Kmeans.


##### ***Caso 1: 2 clusters (k=2)***

&nbsp;

Se ejecuta el método "Kmeans" con los centroides obtenidos con el jerárquico

```{r, include=TRUE, result="hold", warning=TRUE}

kmeans_k2 <- kmeans(crime_df3,centers=centroJer_k2[,3:7])
kmeans_k2$centers

```
&nbsp;

Calculamos la distribución porcentual de los elementos en cada grupo del método de optimización (cluster):

```{r, include=TRUE, result="hold", warning=TRUE}

perc_tamKms_Cluster_k2 <- kmeans_k2$size/sum(kmeans_k2$size)
perc_tamKms_Cluster_k2

```


&nbsp;


##### ***Caso 2: 6 clusters (k=6)***

&nbsp;

Se ejecuta el método "Kmeans" con los centroides obtenidos con el jerárquico.

```{r, include=TRUE, result="hold", warning=TRUE}

kmeans_k6 <- kmeans(crime_df3,centers=centroJer_k6[,3:7])
kmeans_k6$centers

```


Calculamos la distribución porcentual de los elementos en cada grupo del método de optimización (cluster):

```{r, include=TRUE, result="hold", warning=TRUE}

perc_tamKms_Cluster_k6 <- kmeans_k6$size/sum(kmeans_k6$size)
perc_tamKms_Cluster_k6

```

&nbsp;

#### 3.5. VISUALIZACIÓN DE LOS RESULTADOS

##### ***Caso 1: 2 clusters (k=2)***

&nbsp;

Centroides de los clusters definidos por ambos métodos.

- Método jerárquico:
```{r, include=TRUE, result="hold", warning=TRUE}

print.data.frame(centroJer_k2)

```

- Método Kmeans:
```{r, include=TRUE, result="hold", warning=TRUE}

cbind(Cluster_k2=c(1:2),tamano_Cluster=kmeans_k2$size,kmeans_k2$centers)

```


Distribución porcentual de los elementos en cada grupo de ambos métodos.

- Método jerárquico:
```{r, include=TRUE, result="hold", warning=TRUE}

perc_tamJer_Cluster_k2

```

- Método Kmeans:
```{r, include=TRUE, result="hold", warning=TRUE}

perc_tamKms_Cluster_k2

```

&nbsp;

Obtenemos una primera visualización gráfica de los clusters definidos (K=2):

```{r, include=TRUE, result="hold", warning=TRUE, fig.width=18, fig.height=14}

fviz_cluster(kmeans_k2, data=crime_df3, ellipse.type="convex") + theme_minimal() + ggtitle("k = 2") 

```

&nbsp;

Para comparar y revisar las diferencias de los centroides obtenidos mediante ambos métodos (jerárquico y Kmeans) vamos a representar los centroides medios (media de todos los centroides) de cada método.

```{r, include=TRUE, result="hold", warning=TRUE, fig.width=18, fig.height=11.5}

centrOpt_k2 <- kmeans_k2$centers

tamanoClusters <- sqldf("SELECT cluster_k2, 
                       count(*) AS tamano_Cluster from asignJer_k2
                       GROUP BY cluster_k2")

centrOptRadar_k2 <- rbind(
  rep(7,5) , 
  rep(0,5) , 
  apply(centroJer_k2[,3:7], 2, mean),
  apply(centrOpt_k2, 2, mean),
  centrOpt_k2)

colors_border = c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in = c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )
  
radarchart( as.data.frame(centrOptRadar_k2[c(1:4),])  , axistype=1 , 
            #custom polygon
            pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
            #custom the grid
            cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
            #custom labels
            vlcex=0.8,
          )

```
Como se puede apreciar no hay prácticamente diferencias entre los mismos.

Por último, vamos a representamos los centroides de los clusters obtenidos con el método Kmeans. Ésta es una forma interesante para analizar y describir los grupos de elementos obtenidos.

```{r, include=TRUE, result="hold", warning=TRUE, fig.width=18, fig.height=9}

par(mfrow=c(1,2))

for (i in 5:nrow(centrOptRadar_k2))
{
  tamano <- tamanoClusters[i-4,2]
  
  radarchart( as.data.frame(centrOptRadar_k2[c(1:3,i),])  , axistype=1 , 
              #custom polygon
              pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
              #custom the grid
              cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
              #custom labels
              vlcex=0.8,
              title=paste0("Tamano:",tamano)
  )
}

```


##### ***Caso 2: 6 clusters (k=6)***

&nbsp;

Centroides de los clusters definidos por ambos métodos:

- Método jerárquico:
```{r, include=TRUE, result="hold", warning=TRUE}

print.data.frame(centroJer_k6)

```

- Método Kmeans:
```{r, include=TRUE, result="hold", warning=TRUE}

cbind(Cluster_k6=c(1:6),tamano_Cluster=kmeans_k6$size,kmeans_k6$centers)

```


Distribución porcentual de los elementos en cada grupo de ambos métodos:

- Método jerárquico:
```{r, include=TRUE, result="hold", warning=TRUE}

perc_tamJer_Cluster_k6

```

- Método Kmeans:
```{r, include=TRUE, result="hold", warning=TRUE}

perc_tamKms_Cluster_k6

```

&nbsp;

Obtenemos una primera visualización gráfica de los clusters definidos (K=6):

```{r, include=TRUE, result="hold", warning=TRUE, fig.width=18, fig.height=12.8}

fviz_cluster(kmeans_k6, data=crime_df3, ellipse.type="convex") + theme_minimal() + ggtitle("k = 6") 

```

&nbsp;

Para comparar y revisar las diferencias de los centroides obtenidos mediante ambos métodos (jerárquico y Kmeans) vamos a representar los centroides medios (media de todos los centroides) de cada método.

```{r, include=TRUE, result="hold", warning=TRUE, fig.width=18, fig.height=11.5}

centrOpt_k6 <- kmeans_k6$centers
tamanoClusters <- sqldf("SELECT cluster_k6, 
                       count(*) AS tamano_Cluster from asignJer_k6
                       GROUP BY cluster_k6")

centrOptRadar_k6 <- rbind(
  rep(7,5), rep(0,5) , 
  apply(centroJer_k6[,3:7], 2, mean),
  apply(centrOpt_k6, 2, mean), centrOpt_k6)

colors_border = c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in = c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )
  
radarchart( as.data.frame(centrOptRadar_k6[c(1:4),])  , axistype=1 , 
            #custom polygon
            pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
            #custom the grid
            cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
            #custom labels
            vlcex=0.8
    )

```
Como se puede apreciar no hay prácticamente diferencias entre los mismos.

Por último, vamos a representamos los centroides de los clusters obtenidos con el método Kmeans. Ésta es una forma interesante para analizar y describir los grupos de elementos obtenidos.

```{r, include=TRUE, result="hold", warning=TRUE, fig.width=15, fig.height=6.9}

par(mfrow=c(1,2))

for (i in 5:nrow(centrOptRadar_k6))
{
  tamano <- tamanoClusters[i-4,2]
  
  radarchart( as.data.frame(centrOptRadar_k6[c(1:3,i),])  , axistype=1 , 
              #custom polygon
              pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
              #custom the grid
              cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
              #custom labels
              vlcex=0.8,
              title=paste0("Tamano:",tamano)
  )
}

```


&nbsp;

#### 3.6. ANÁLISIS DE LOS RESULTADOS

&nbsp;

Analizando los resultados obtenidos de las 2 segregaciones propuestas, ambas son correctas pero se considera que quizás la mejor solución sea la de dividir los datos en 6 clusters (K=6).

Se toma esta decisión final, teniendo en cuenta el orden de magnitud del número de elementos del dataset (dividir los 50 elementos en sólo 2 grupos, quizás sea demasiado simple) y tras el análisis y visualización de los distintos clusters obtenidos en ambos casos, ya que los obtenidos en el segundo caso (k=6) se considera que permiten una diferenciación/clasificación mayor y bastante óptima de los elementos del dataset.

&nbsp;

&nbsp;


*** 

&nbsp;

### 4. CONCLUSIONES

Es importante ser concientes de que para realizar el "clustering" se van a calcular y utilizar las distancias entre elementos por lo que será necesario un análisis y tratamiento preliminar de los datos (missing values, outliers, variables continuas y/o discretas, categorías, distancia entre categorías, transformaciones, estandarización...) teniendo en cuenta el caso en estudio. Posterior a ese análisis también será importante definir la forma de calcular esas distancias entre elementos. 

&nbsp;

Por último, para la determinación del tipo de algoritmo influyen muchos factores pero quizás los que se puedan considerar más influyentes son: el tamaño de los datos y el conocimiento sobre ellos y el área relacionada con los mismos. Ésto nos va a condicionar en muchos casos a tomar un método de un tipo u otro:
  
  - Metodos jerárquicos - se tienen pocos datos datos.
  - Métodos de optimización - se tienen muchos datos, se cuenta con orden aproximado para el número de clusters.

Un método que combina las ventajas de ambos tipos es el método bietápico y es una muy buena estrategia siempre y cuando no se tengan condicionantes específicos de los datos o externos del cliente. Consiste en aplicar primero un método jerárquico para conseguir información sobre el posible número de clusters óptimos para posteriormente, empleando esa información, aplicar un método de optimización para resolver el problema de clustering.

&nbsp;

Todo lo mencionado anteriormente puede servir como guía para recordar elementos importantes del análisis, pero a fin de cuentas el "clustering" es un análisis de tipo no supervisado (no existe una variable "target" que nos permita definir un error de forma exacta) y por tanto es subjetivo. Depende de muchos factores, consideraciones particulares, conocimiento del área/sector del caso y a veces incluso condicionantes externos, por ello lo más importante es realizar el análisis teniendo en cuenta toda esa información, centrándose en el caso concreto de estudio.
